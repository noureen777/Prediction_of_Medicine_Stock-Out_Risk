{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0tchahVQP6z",
    "outputId": "6f3786af-b981-49cc-c4d1-3993f9174e0f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "to build BERT model:\n",
    "\n",
    "\n",
    "1.   tokenization\n",
    "2.   the  DataLoader\n",
    "3.   model initization (pre-trained)\n",
    "4.   optimizer\n",
    "5.   the train loop\n",
    "6.   evaluation\n",
    "\n"
   ],
   "metadata": {
    "id": "dWgzSVyuRXxT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "id": "eLOLw8m2SeX3"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "D3G5u9LNSkRW"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataa Preparation\n",
    "  1. encodes the categories: converts \"storage_issue\" -> 0, etc.\n",
    "  2. tokenizes: converts text -> numbers (input IDs)\n",
    "  3. creates dataLoaders: packages the data into batches of 16 for the model"
   ],
   "metadata": {
    "id": "DwS4ms3dahEM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('text_data_cleaned.csv')"
   ],
   "metadata": {
    "id": "7VIfLVu-bFsJ"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. encode labels (text -> numbers)\n",
    "# the model needs numbers (0, 1, 2...), not strings (\"storage_issue\")\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['category'])"
   ],
   "metadata": {
    "id": "fZ87VbEvcFqj"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the mapping to check later\n",
    "# we will need it as a translator\n",
    "label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label Mapping:\", label_map)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXQKAeL1cTBC",
    "outputId": "070b27e0-eb62-4838-83f3-d6e66cfd646e"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label Mapping: {'contract_dispute': np.int64(0), 'currency_issue': np.int64(1), 'demand_spike': np.int64(2), 'distribution_center_issue': np.int64(3), 'forecasting_error': np.int64(4), 'import_delay': np.int64(5), 'it_system_failure': np.int64(6), 'labor_shortage': np.int64(7), 'manufacturing_issue': np.int64(8), 'neutral_report': np.int64(9), 'quality_control_issue': np.int64(10), 'regulation_issue': np.int64(11), 'seasonal_variation': np.int64(12), 'storage_issue': np.int64(13), 'supplier_problem': np.int64(14), 'transport_issue': np.int64(15)}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#split the data (train 80% - validation 20%)\n",
    "# we use 'stratify' to ensure all categories are represented in both sets\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    df['text_clean'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ],
   "metadata": {
    "id": "P8RLjIS-cZVS"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. tokenizes\n",
    "# initialize BERT tokenizer\n",
    "\n",
    "#bert-base --> this is the standard version (12 Layers)\n",
    "#uncased --> ignore capitalized \"wez\"and \"Wez\"are the same\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "9e345eea91be4d4e81c6abf0efedfbe3",
      "fd397410997f49018a0c7466575a0115",
      "033ed8e970774a0687eb9b6be16f24b9",
      "9bb0b6f92c774003b7a2901fe0779e47",
      "cf6f2f9108a54ab18594a5183951f4a3",
      "54cc4b2f2cec4bdea7b5252f9ed62484",
      "feb7a6059e764d12aa0dd1da84731d5d",
      "2b8d3e5111374f16ad05b33a92e6b9ee",
      "db7adfd928e34ebd9942304702061a79",
      "9f555e821cff4a01a712754a9e9104cd",
      "96427987e8594fb7a704210f39237bcb",
      "54d1033e54544ecc9f2524248017d38f",
      "ba72e897fde847908a7ae75a7a4bc147",
      "b6b69fa603444e31b3c58ff6fe2a9246",
      "98002567a8cb4f48a6bf8ef6705f2c7d",
      "f31f02c24cb54a11bdd6d8ae0363f9f1",
      "40a43435d55c4521af97410b4298cc26",
      "d3e47c6634fa457fae78ae099cd394cf",
      "d17390c0128b408caad33a3d9947b88a",
      "c6f1ba37118941afa0c30c2f3fa463a4",
      "2aed8c37f495450895c6f93f60dd85f9",
      "15ae8bbd52354e71b5c44250bf69a0ac",
      "2bd98d3ea6dc4dfb88818c81a5aea5ce",
      "72eb385496f5451bad98323f16064f69",
      "e3df8536f6344716a4b04d07bc18d072",
      "e91898b8c8c6425db68e1e037c71838f",
      "4cc558daa51e47049a9ff3afdfeb93ef",
      "0f43ab609aea4f08bf951d6b22ecf264",
      "d8893d077296416fb3d80e8cee589f05",
      "ed87f8997a8f4f21911f40a8ee9a711d",
      "bb6beda01b4c40f98ed0a89a509817cf",
      "13b225a5b8624945accbc9962a7b42e0",
      "26647012ff3f4f70880f7126b8ebcb08",
      "aec6104cb6af4800b862fb316e047218",
      "d10ad48a34e94d6e9c7e05b6c3b3ab58",
      "db932a6c816a4068b8fed6272a461cfe",
      "3c4e3c502d614245acfb9fcf529079a9",
      "504b8f5e0e904d71b625097594741b6d",
      "ae6eedd06cb949afa70ef7e2ad77da06",
      "8e30e684c11a421ca1dac21aaefa533c",
      "f26d292fe4294326b7a9774fae118d7f",
      "7f859adf6e7a4f22b5d41e6eaeba504e",
      "ba0a12e850e44dd88b60044f697a427c",
      "ae3e7dd86d84451191a08cdd1ad08a51"
     ]
    },
    "id": "ROx-4K5odb7A",
    "outputId": "037ca2c0-296f-47d2-f4a1-ed2b48549e3a"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e345eea91be4d4e81c6abf0efedfbe3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54d1033e54544ecc9f2524248017d38f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bd98d3ea6dc4dfb88818c81a5aea5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec6104cb6af4800b862fb316e047218"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenization function\n",
    "def tokenize_data(text_list, labels):\n",
    "    encoded_batch = tokenizer.batch_encode_plus(\n",
    "        text_list.tolist(),\n",
    "        add_special_tokens=True,      # add [CLS] and [SEP]\n",
    "        padding='max_length',         # pad to max length\n",
    "        truncation=True,              # truncate if too long\n",
    "        max_length=64,                # fixed length for all sentences\n",
    "        return_attention_mask=True,   # generate masks\n",
    "        return_tensors='pt'           # return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return TensorDataset(\n",
    "        encoded_batch['input_ids'],\n",
    "        encoded_batch['attention_mask'],\n",
    "        torch.tensor(labels.tolist())\n",
    "    )"
   ],
   "metadata": {
    "id": "_y9nZ5DdeSB6"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#create datasets\n",
    "train_dataset = tokenize_data(X_train_text, y_train)\n",
    "val_dataset = tokenize_data(X_val_text, y_val)"
   ],
   "metadata": {
    "id": "TdfZcLoklNlp"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"train_dataset:\", train_dataset)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPSIW7o9lc_a",
    "outputId": "09553662-d734-4849-90e8-430c0bc815f7"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_dataset: <torch.utils.data.dataset.TensorDataset object at 0x7a3778ff9e20>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids, attention_masks, labels = train_dataset[0]\n",
    "\n",
    "print(\"1. Input IDs (The Numbers):\", input_ids)\n",
    "print(\"2. Label (The Answer):\", labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6z-v8OGnnWCd",
    "outputId": "c1b65704-0932-4c03-d497-163bc0078044"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1. Input IDs (The Numbers): tensor([  101,  6887, 27292, 20499,  1999,  2047,  3923,  4279,  2988,  3314,\n",
      "         2007,  3424,  3597,  8490,  7068,  7666,  1012,  2027,  6563,  4026,\n",
      "         1998,  3665,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "2. Label (The Answer): tensor(15)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#3. dataloader\n",
    "batch_size = 16"
   ],
   "metadata": {
    "id": "BW0p8xXRnpaV"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset), # shuffle training data\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset), # don't shuffle validation\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\":D! Data is tokenized and packed into DataLoaders.\")\n",
    "print(f\"Training Batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation Batches: {len(validation_dataloader)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QpYpXO2oryn",
    "outputId": "639f8ba1-9679-4657-96ab-811c260b3578"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ":D! Data is tokenized and packed into DataLoaders.\n",
      "Training Batches: 20\n",
      "Validation Batches: 5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2yVrJXY3pC4x"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "part 2: model initializing"
   ],
   "metadata": {
    "id": "TAPTykeGvSmf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ],
   "metadata": {
    "id": "V9WZShmY0cCQ"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#initizalize the mode\n",
    "print(\"initializing BERT model\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label_map),  # Automatically set number of categories\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124,
     "referenced_widgets": [
      "e25d7df5a03b44808e3128b68e0b09e4",
      "ec17a82b14de430d9064ab6563eb0c33",
      "d074948c923344ba904a4c98dcb4a181",
      "44ecf3b40da6413396a44e9acef4a06d",
      "7c1a64f2926f4f46bf19b32a48f7b6f7",
      "2d8a77d48fb644a1af4bdcab2464a4e4",
      "fd65f2bdf4c44bd989cee082ae027e38",
      "003930ab711840d68199e6d27595dc0b",
      "f12ca0569ec34605ae226f4feb5b4381",
      "634646f1657849ffb6cf20743059946d",
      "3e421a27af3b428391e90e68cbad36cc"
     ]
    },
    "id": "pfVrvSYfM-G3",
    "outputId": "e36f4f64-1fa8-4e66-eb62-f211161adf2f"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initializing BERT model\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e25d7df5a03b44808e3128b68e0b09e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# move to GPU and GPU are built for the massive math required by neural networks\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"   Model loaded on: {device}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5J6Wv3rNilp",
    "outputId": "3b37cbda-9692-4c70-b4cd-864adce2ea32"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Model loaded on: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "part 3: optimizer"
   ],
   "metadata": {
    "id": "xn2jgGLNQ9WT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup the Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ],
   "metadata": {
    "id": "1jRGUc5lN4oO"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)"
   ],
   "metadata": {
    "id": "bF1h-lvTQShT"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "part 4: training looop"
   ],
   "metadata": {
    "id": "s6Z4_MM8REw0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n2. Starting Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n   Epoch {epoch + 1} / {epochs}\")\n",
    "\n",
    "    # --- TRAINING PHASE ---\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, labels = batch\n",
    "\n",
    "        # The Learning Steps\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "\n",
    "        # Print Loss every 5 batches so we can watch it learn\n",
    "        loss = output.loss\n",
    "        if step % 5 == 0 and step > 0:\n",
    "             print(f\"      Batch {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask=input_mask)\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        predictions = torch.argmax(output.logits, dim=1)\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    # Print Score\n",
    "    accuracy = total_correct / len(val_dataset)\n",
    "    print(f\"   Validation Accuracy: {accuracy:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRa8raQhSwk1",
    "outputId": "1b3f3f38-4745-445f-dabd-dbdb5e3f3059"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "2. Starting Training...\n",
      "\n",
      "   Epoch 1 / 10\n",
      "      Batch 5: Loss = 2.4921\n",
      "      Batch 10: Loss = 1.8643\n",
      "      Batch 15: Loss = 1.8200\n",
      "   Validation Accuracy: 0.38\n",
      "\n",
      "   Epoch 2 / 10\n",
      "      Batch 5: Loss = 2.3156\n",
      "      Batch 10: Loss = 2.0465\n",
      "      Batch 15: Loss = 2.2461\n",
      "   Validation Accuracy: 0.40\n",
      "\n",
      "   Epoch 3 / 10\n",
      "      Batch 5: Loss = 2.0417\n",
      "      Batch 10: Loss = 1.9814\n",
      "      Batch 15: Loss = 1.7841\n",
      "   Validation Accuracy: 0.45\n",
      "\n",
      "   Epoch 4 / 10\n",
      "      Batch 5: Loss = 1.9750\n",
      "      Batch 10: Loss = 1.5148\n",
      "      Batch 15: Loss = 1.5577\n",
      "   Validation Accuracy: 0.59\n",
      "\n",
      "   Epoch 5 / 10\n",
      "      Batch 5: Loss = 1.5785\n",
      "      Batch 10: Loss = 1.3087\n",
      "      Batch 15: Loss = 1.5314\n",
      "   Validation Accuracy: 0.75\n",
      "\n",
      "   Epoch 6 / 10\n",
      "      Batch 5: Loss = 1.6424\n",
      "      Batch 10: Loss = 1.4791\n",
      "      Batch 15: Loss = 1.3148\n",
      "   Validation Accuracy: 0.88\n",
      "\n",
      "   Epoch 7 / 10\n",
      "      Batch 5: Loss = 1.4475\n",
      "      Batch 10: Loss = 1.4292\n",
      "      Batch 15: Loss = 1.1313\n",
      "   Validation Accuracy: 0.90\n",
      "\n",
      "   Epoch 8 / 10\n",
      "      Batch 5: Loss = 1.0840\n",
      "      Batch 10: Loss = 1.2584\n",
      "      Batch 15: Loss = 1.2100\n",
      "   Validation Accuracy: 0.94\n",
      "\n",
      "   Epoch 9 / 10\n",
      "      Batch 5: Loss = 1.0306\n",
      "      Batch 10: Loss = 1.1161\n",
      "      Batch 15: Loss = 1.0676\n",
      "   Validation Accuracy: 0.94\n",
      "\n",
      "   Epoch 10 / 10\n",
      "      Batch 5: Loss = 1.2108\n",
      "      Batch 10: Loss = 1.1221\n",
      "      Batch 15: Loss = 1.1622\n",
      "   Validation Accuracy: 0.93\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#model is now understanding 94/100 pharmacy reports correctly.\n"
   ],
   "metadata": {
    "id": "Y6TV3lt7Rogj"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}